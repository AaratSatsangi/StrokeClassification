=========================Layer Names=========================
0: model.cls_token
1: model.pos_embed
2: model.patch_embed.proj.weight
3: model.patch_embed.proj.bias
4: model.blocks.0.norm1.weight
5: model.blocks.0.norm1.bias
6: model.blocks.0.attn.gating_param
7: model.blocks.0.attn.qk.weight
8: model.blocks.0.attn.v.weight
9: model.blocks.0.attn.proj.weight
10: model.blocks.0.attn.proj.bias
11: model.blocks.0.attn.pos_proj.weight
12: model.blocks.0.attn.pos_proj.bias
13: model.blocks.0.norm2.weight
14: model.blocks.0.norm2.bias
15: model.blocks.0.mlp.fc1.weight
16: model.blocks.0.mlp.fc1.bias
17: model.blocks.0.mlp.fc2.weight
18: model.blocks.0.mlp.fc2.bias
19: model.blocks.1.norm1.weight
20: model.blocks.1.norm1.bias
21: model.blocks.1.attn.gating_param
22: model.blocks.1.attn.qk.weight
23: model.blocks.1.attn.v.weight
24: model.blocks.1.attn.proj.weight
25: model.blocks.1.attn.proj.bias
26: model.blocks.1.attn.pos_proj.weight
27: model.blocks.1.attn.pos_proj.bias
28: model.blocks.1.norm2.weight
29: model.blocks.1.norm2.bias
30: model.blocks.1.mlp.fc1.weight
31: model.blocks.1.mlp.fc1.bias
32: model.blocks.1.mlp.fc2.weight
33: model.blocks.1.mlp.fc2.bias
34: model.blocks.2.norm1.weight
35: model.blocks.2.norm1.bias
36: model.blocks.2.attn.gating_param
37: model.blocks.2.attn.qk.weight
38: model.blocks.2.attn.v.weight
39: model.blocks.2.attn.proj.weight
40: model.blocks.2.attn.proj.bias
41: model.blocks.2.attn.pos_proj.weight
42: model.blocks.2.attn.pos_proj.bias
43: model.blocks.2.norm2.weight
44: model.blocks.2.norm2.bias
45: model.blocks.2.mlp.fc1.weight
46: model.blocks.2.mlp.fc1.bias
47: model.blocks.2.mlp.fc2.weight
48: model.blocks.2.mlp.fc2.bias
49: model.blocks.3.norm1.weight
50: model.blocks.3.norm1.bias
51: model.blocks.3.attn.gating_param
52: model.blocks.3.attn.qk.weight
53: model.blocks.3.attn.v.weight
54: model.blocks.3.attn.proj.weight
55: model.blocks.3.attn.proj.bias
56: model.blocks.3.attn.pos_proj.weight
57: model.blocks.3.attn.pos_proj.bias
58: model.blocks.3.norm2.weight
59: model.blocks.3.norm2.bias
60: model.blocks.3.mlp.fc1.weight
61: model.blocks.3.mlp.fc1.bias
62: model.blocks.3.mlp.fc2.weight
63: model.blocks.3.mlp.fc2.bias
64: model.blocks.4.norm1.weight
65: model.blocks.4.norm1.bias
66: model.blocks.4.attn.gating_param
67: model.blocks.4.attn.qk.weight
68: model.blocks.4.attn.v.weight
69: model.blocks.4.attn.proj.weight
70: model.blocks.4.attn.proj.bias
71: model.blocks.4.attn.pos_proj.weight
72: model.blocks.4.attn.pos_proj.bias
73: model.blocks.4.norm2.weight
74: model.blocks.4.norm2.bias
75: model.blocks.4.mlp.fc1.weight
76: model.blocks.4.mlp.fc1.bias
77: model.blocks.4.mlp.fc2.weight
78: model.blocks.4.mlp.fc2.bias
79: model.blocks.5.norm1.weight
80: model.blocks.5.norm1.bias
81: model.blocks.5.attn.gating_param
82: model.blocks.5.attn.qk.weight
83: model.blocks.5.attn.v.weight
84: model.blocks.5.attn.proj.weight
85: model.blocks.5.attn.proj.bias
86: model.blocks.5.attn.pos_proj.weight
87: model.blocks.5.attn.pos_proj.bias
88: model.blocks.5.norm2.weight
89: model.blocks.5.norm2.bias
90: model.blocks.5.mlp.fc1.weight
91: model.blocks.5.mlp.fc1.bias
92: model.blocks.5.mlp.fc2.weight
93: model.blocks.5.mlp.fc2.bias
94: model.blocks.6.norm1.weight
95: model.blocks.6.norm1.bias
96: model.blocks.6.attn.gating_param
97: model.blocks.6.attn.qk.weight
98: model.blocks.6.attn.v.weight
99: model.blocks.6.attn.proj.weight
100: model.blocks.6.attn.proj.bias
101: model.blocks.6.attn.pos_proj.weight
102: model.blocks.6.attn.pos_proj.bias
103: model.blocks.6.norm2.weight
104: model.blocks.6.norm2.bias
105: model.blocks.6.mlp.fc1.weight
106: model.blocks.6.mlp.fc1.bias
107: model.blocks.6.mlp.fc2.weight
108: model.blocks.6.mlp.fc2.bias
109: model.blocks.7.norm1.weight
110: model.blocks.7.norm1.bias
111: model.blocks.7.attn.gating_param
112: model.blocks.7.attn.qk.weight
113: model.blocks.7.attn.v.weight
114: model.blocks.7.attn.proj.weight
115: model.blocks.7.attn.proj.bias
116: model.blocks.7.attn.pos_proj.weight
117: model.blocks.7.attn.pos_proj.bias
118: model.blocks.7.norm2.weight
119: model.blocks.7.norm2.bias
120: model.blocks.7.mlp.fc1.weight
121: model.blocks.7.mlp.fc1.bias
122: model.blocks.7.mlp.fc2.weight
123: model.blocks.7.mlp.fc2.bias		(freezed till here)
124: model.blocks.8.norm1.weight
125: model.blocks.8.norm1.bias
126: model.blocks.8.attn.gating_param
127: model.blocks.8.attn.qk.weight
128: model.blocks.8.attn.v.weight
129: model.blocks.8.attn.proj.weight
130: model.blocks.8.attn.proj.bias
131: model.blocks.8.attn.pos_proj.weight
132: model.blocks.8.attn.pos_proj.bias
133: model.blocks.8.norm2.weight
134: model.blocks.8.norm2.bias
135: model.blocks.8.mlp.fc1.weight
136: model.blocks.8.mlp.fc1.bias
137: model.blocks.8.mlp.fc2.weight
138: model.blocks.8.mlp.fc2.bias
139: model.blocks.9.norm1.weight
140: model.blocks.9.norm1.bias
141: model.blocks.9.attn.gating_param
142: model.blocks.9.attn.qk.weight
143: model.blocks.9.attn.v.weight
144: model.blocks.9.attn.proj.weight
145: model.blocks.9.attn.proj.bias
146: model.blocks.9.attn.pos_proj.weight
147: model.blocks.9.attn.pos_proj.bias
148: model.blocks.9.norm2.weight
149: model.blocks.9.norm2.bias
150: model.blocks.9.mlp.fc1.weight
151: model.blocks.9.mlp.fc1.bias
152: model.blocks.9.mlp.fc2.weight
153: model.blocks.9.mlp.fc2.bias
154: model.blocks.10.norm1.weight
155: model.blocks.10.norm1.bias
156: model.blocks.10.attn.qkv.weight
157: model.blocks.10.attn.proj.weight
158: model.blocks.10.attn.proj.bias
159: model.blocks.10.norm2.weight
160: model.blocks.10.norm2.bias
161: model.blocks.10.mlp.fc1.weight
162: model.blocks.10.mlp.fc1.bias
163: model.blocks.10.mlp.fc2.weight
164: model.blocks.10.mlp.fc2.bias
165: model.blocks.11.norm1.weight
166: model.blocks.11.norm1.bias
167: model.blocks.11.attn.qkv.weight
168: model.blocks.11.attn.proj.weight
169: model.blocks.11.attn.proj.bias
170: model.blocks.11.norm2.weight
171: model.blocks.11.norm2.bias
172: model.blocks.11.mlp.fc1.weight
173: model.blocks.11.mlp.fc1.bias
174: model.blocks.11.mlp.fc2.weight
175: model.blocks.11.mlp.fc2.bias
176: model.norm.weight
177: model.norm.bias
178: model.head.weight
179: model.head.bias
=============================================================


========================================================================================================================
Layer (type:depth-idx)                        Input Shape               Output Shape              Param #
========================================================================================================================
CvT                                           [1, 1, 224, 224]          [1, 3]                    --
├─ConVit: 1-1                                 [1, 1, 224, 224]          [1, 3]                    151,296
│    └─PatchEmbed: 2-1                        [1, 1, 224, 224]          [1, 196, 768]             --
│    │    └─Conv2d: 3-1                       [1, 1, 224, 224]          [1, 768, 14, 14]          197,376
│    │    └─Identity: 3-2                     [1, 196, 768]             [1, 196, 768]             --
│    └─Dropout: 2-2                           [1, 196, 768]             [1, 196, 768]             --
│    └─ModuleList: 2-3                        --                        --                        --
│    │    └─Block: 3-3                        [1, 196, 768]             [1, 196, 768]             (7,085,648)
│    │    └─Block: 3-4                        [1, 196, 768]             [1, 196, 768]             (7,085,648)
│    │    └─Block: 3-5                        [1, 196, 768]             [1, 196, 768]             (7,085,648)
│    │    └─Block: 3-6                        [1, 196, 768]             [1, 196, 768]             (7,085,648)
│    │    └─Block: 3-7                        [1, 196, 768]             [1, 196, 768]             (7,085,648)
│    │    └─Block: 3-8                        [1, 196, 768]             [1, 196, 768]             (7,085,648)
│    │    └─Block: 3-9                        [1, 196, 768]             [1, 196, 768]             (7,085,648)
│    │    └─Block: 3-10                       [1, 196, 768]             [1, 196, 768]             7,085,648
│    │    └─Block: 3-11                       [1, 196, 768]             [1, 196, 768]             7,085,648
│    │    └─Block: 3-12                       [1, 196, 768]             [1, 196, 768]             7,085,648
│    │    └─Block: 3-13                       [1, 197, 768]             [1, 197, 768]             7,085,568
│    │    └─Block: 3-14                       [1, 197, 768]             [1, 197, 768]             7,085,568
│    └─LayerNorm: 2-4                         [1, 197, 768]             [1, 197, 768]             1,536
│    └─Dropout: 2-5                           [1, 768]                  [1, 768]                  --
│    └─Linear: 2-6                            [1, 768]                  [1, 3]                    2,307
========================================================================================================================
Total params: 85,380,131
Trainable params: 28,544,419
Non-trainable params: 56,835,712
Total mult-adds (M): 123.72
========================================================================================================================
Input size (MB): 0.20
Forward/backward pass size (MB): 210.68
Params size (MB): 340.91
Estimated Total Size (MB): 551.80
========================================================================================================================