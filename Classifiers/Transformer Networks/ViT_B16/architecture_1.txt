=========================Layer Names=========================
0: vitb16.class_token
1: vitb16.conv_proj.weight
2: vitb16.conv_proj.bias
3: vitb16.encoder.pos_embedding
4: vitb16.encoder.layers.encoder_layer_0.ln_1.weight
5: vitb16.encoder.layers.encoder_layer_0.ln_1.bias
6: vitb16.encoder.layers.encoder_layer_0.self_attention.in_proj_weight
7: vitb16.encoder.layers.encoder_layer_0.self_attention.in_proj_bias
8: vitb16.encoder.layers.encoder_layer_0.self_attention.out_proj.weight
9: vitb16.encoder.layers.encoder_layer_0.self_attention.out_proj.bias
10: vitb16.encoder.layers.encoder_layer_0.ln_2.weight
11: vitb16.encoder.layers.encoder_layer_0.ln_2.bias
12: vitb16.encoder.layers.encoder_layer_0.mlp.0.weight
13: vitb16.encoder.layers.encoder_layer_0.mlp.0.bias
14: vitb16.encoder.layers.encoder_layer_0.mlp.3.weight
15: vitb16.encoder.layers.encoder_layer_0.mlp.3.bias
16: vitb16.encoder.layers.encoder_layer_1.ln_1.weight	(freezed till here)
17: vitb16.encoder.layers.encoder_layer_1.ln_1.bias
18: vitb16.encoder.layers.encoder_layer_1.self_attention.in_proj_weight
19: vitb16.encoder.layers.encoder_layer_1.self_attention.in_proj_bias
20: vitb16.encoder.layers.encoder_layer_1.self_attention.out_proj.weight
21: vitb16.encoder.layers.encoder_layer_1.self_attention.out_proj.bias
22: vitb16.encoder.layers.encoder_layer_1.ln_2.weight
23: vitb16.encoder.layers.encoder_layer_1.ln_2.bias
24: vitb16.encoder.layers.encoder_layer_1.mlp.0.weight
25: vitb16.encoder.layers.encoder_layer_1.mlp.0.bias
26: vitb16.encoder.layers.encoder_layer_1.mlp.3.weight
27: vitb16.encoder.layers.encoder_layer_1.mlp.3.bias
28: vitb16.encoder.layers.encoder_layer_2.ln_1.weight
29: vitb16.encoder.layers.encoder_layer_2.ln_1.bias
30: vitb16.encoder.layers.encoder_layer_2.self_attention.in_proj_weight
31: vitb16.encoder.layers.encoder_layer_2.self_attention.in_proj_bias
32: vitb16.encoder.layers.encoder_layer_2.self_attention.out_proj.weight
33: vitb16.encoder.layers.encoder_layer_2.self_attention.out_proj.bias
34: vitb16.encoder.layers.encoder_layer_2.ln_2.weight
35: vitb16.encoder.layers.encoder_layer_2.ln_2.bias
36: vitb16.encoder.layers.encoder_layer_2.mlp.0.weight
37: vitb16.encoder.layers.encoder_layer_2.mlp.0.bias
38: vitb16.encoder.layers.encoder_layer_2.mlp.3.weight
39: vitb16.encoder.layers.encoder_layer_2.mlp.3.bias
40: vitb16.encoder.layers.encoder_layer_3.ln_1.weight
41: vitb16.encoder.layers.encoder_layer_3.ln_1.bias
42: vitb16.encoder.layers.encoder_layer_3.self_attention.in_proj_weight
43: vitb16.encoder.layers.encoder_layer_3.self_attention.in_proj_bias
44: vitb16.encoder.layers.encoder_layer_3.self_attention.out_proj.weight
45: vitb16.encoder.layers.encoder_layer_3.self_attention.out_proj.bias
46: vitb16.encoder.layers.encoder_layer_3.ln_2.weight
47: vitb16.encoder.layers.encoder_layer_3.ln_2.bias
48: vitb16.encoder.layers.encoder_layer_3.mlp.0.weight
49: vitb16.encoder.layers.encoder_layer_3.mlp.0.bias
50: vitb16.encoder.layers.encoder_layer_3.mlp.3.weight
51: vitb16.encoder.layers.encoder_layer_3.mlp.3.bias
52: vitb16.encoder.layers.encoder_layer_4.ln_1.weight
53: vitb16.encoder.layers.encoder_layer_4.ln_1.bias
54: vitb16.encoder.layers.encoder_layer_4.self_attention.in_proj_weight
55: vitb16.encoder.layers.encoder_layer_4.self_attention.in_proj_bias
56: vitb16.encoder.layers.encoder_layer_4.self_attention.out_proj.weight
57: vitb16.encoder.layers.encoder_layer_4.self_attention.out_proj.bias
58: vitb16.encoder.layers.encoder_layer_4.ln_2.weight
59: vitb16.encoder.layers.encoder_layer_4.ln_2.bias
60: vitb16.encoder.layers.encoder_layer_4.mlp.0.weight
61: vitb16.encoder.layers.encoder_layer_4.mlp.0.bias
62: vitb16.encoder.layers.encoder_layer_4.mlp.3.weight
63: vitb16.encoder.layers.encoder_layer_4.mlp.3.bias
64: vitb16.encoder.layers.encoder_layer_5.ln_1.weight
65: vitb16.encoder.layers.encoder_layer_5.ln_1.bias
66: vitb16.encoder.layers.encoder_layer_5.self_attention.in_proj_weight
67: vitb16.encoder.layers.encoder_layer_5.self_attention.in_proj_bias
68: vitb16.encoder.layers.encoder_layer_5.self_attention.out_proj.weight
69: vitb16.encoder.layers.encoder_layer_5.self_attention.out_proj.bias
70: vitb16.encoder.layers.encoder_layer_5.ln_2.weight
71: vitb16.encoder.layers.encoder_layer_5.ln_2.bias
72: vitb16.encoder.layers.encoder_layer_5.mlp.0.weight
73: vitb16.encoder.layers.encoder_layer_5.mlp.0.bias
74: vitb16.encoder.layers.encoder_layer_5.mlp.3.weight
75: vitb16.encoder.layers.encoder_layer_5.mlp.3.bias
76: vitb16.encoder.layers.encoder_layer_6.ln_1.weight
77: vitb16.encoder.layers.encoder_layer_6.ln_1.bias
78: vitb16.encoder.layers.encoder_layer_6.self_attention.in_proj_weight
79: vitb16.encoder.layers.encoder_layer_6.self_attention.in_proj_bias
80: vitb16.encoder.layers.encoder_layer_6.self_attention.out_proj.weight
81: vitb16.encoder.layers.encoder_layer_6.self_attention.out_proj.bias
82: vitb16.encoder.layers.encoder_layer_6.ln_2.weight
83: vitb16.encoder.layers.encoder_layer_6.ln_2.bias
84: vitb16.encoder.layers.encoder_layer_6.mlp.0.weight
85: vitb16.encoder.layers.encoder_layer_6.mlp.0.bias
86: vitb16.encoder.layers.encoder_layer_6.mlp.3.weight
87: vitb16.encoder.layers.encoder_layer_6.mlp.3.bias
88: vitb16.encoder.layers.encoder_layer_7.ln_1.weight
89: vitb16.encoder.layers.encoder_layer_7.ln_1.bias
90: vitb16.encoder.layers.encoder_layer_7.self_attention.in_proj_weight
91: vitb16.encoder.layers.encoder_layer_7.self_attention.in_proj_bias
92: vitb16.encoder.layers.encoder_layer_7.self_attention.out_proj.weight
93: vitb16.encoder.layers.encoder_layer_7.self_attention.out_proj.bias
94: vitb16.encoder.layers.encoder_layer_7.ln_2.weight
95: vitb16.encoder.layers.encoder_layer_7.ln_2.bias
96: vitb16.encoder.layers.encoder_layer_7.mlp.0.weight
97: vitb16.encoder.layers.encoder_layer_7.mlp.0.bias
98: vitb16.encoder.layers.encoder_layer_7.mlp.3.weight
99: vitb16.encoder.layers.encoder_layer_7.mlp.3.bias
100: vitb16.encoder.layers.encoder_layer_8.ln_1.weight
101: vitb16.encoder.layers.encoder_layer_8.ln_1.bias
102: vitb16.encoder.layers.encoder_layer_8.self_attention.in_proj_weight
103: vitb16.encoder.layers.encoder_layer_8.self_attention.in_proj_bias
104: vitb16.encoder.layers.encoder_layer_8.self_attention.out_proj.weight
105: vitb16.encoder.layers.encoder_layer_8.self_attention.out_proj.bias
106: vitb16.encoder.layers.encoder_layer_8.ln_2.weight
107: vitb16.encoder.layers.encoder_layer_8.ln_2.bias
108: vitb16.encoder.layers.encoder_layer_8.mlp.0.weight
109: vitb16.encoder.layers.encoder_layer_8.mlp.0.bias
110: vitb16.encoder.layers.encoder_layer_8.mlp.3.weight
111: vitb16.encoder.layers.encoder_layer_8.mlp.3.bias
112: vitb16.encoder.layers.encoder_layer_9.ln_1.weight
113: vitb16.encoder.layers.encoder_layer_9.ln_1.bias
114: vitb16.encoder.layers.encoder_layer_9.self_attention.in_proj_weight
115: vitb16.encoder.layers.encoder_layer_9.self_attention.in_proj_bias
116: vitb16.encoder.layers.encoder_layer_9.self_attention.out_proj.weight
117: vitb16.encoder.layers.encoder_layer_9.self_attention.out_proj.bias
118: vitb16.encoder.layers.encoder_layer_9.ln_2.weight
119: vitb16.encoder.layers.encoder_layer_9.ln_2.bias
120: vitb16.encoder.layers.encoder_layer_9.mlp.0.weight
121: vitb16.encoder.layers.encoder_layer_9.mlp.0.bias
122: vitb16.encoder.layers.encoder_layer_9.mlp.3.weight
123: vitb16.encoder.layers.encoder_layer_9.mlp.3.bias
124: vitb16.encoder.layers.encoder_layer_10.ln_1.weight
125: vitb16.encoder.layers.encoder_layer_10.ln_1.bias
126: vitb16.encoder.layers.encoder_layer_10.self_attention.in_proj_weight
127: vitb16.encoder.layers.encoder_layer_10.self_attention.in_proj_bias
128: vitb16.encoder.layers.encoder_layer_10.self_attention.out_proj.weight
129: vitb16.encoder.layers.encoder_layer_10.self_attention.out_proj.bias
130: vitb16.encoder.layers.encoder_layer_10.ln_2.weight
131: vitb16.encoder.layers.encoder_layer_10.ln_2.bias
132: vitb16.encoder.layers.encoder_layer_10.mlp.0.weight
133: vitb16.encoder.layers.encoder_layer_10.mlp.0.bias
134: vitb16.encoder.layers.encoder_layer_10.mlp.3.weight
135: vitb16.encoder.layers.encoder_layer_10.mlp.3.bias
136: vitb16.encoder.layers.encoder_layer_11.ln_1.weight
137: vitb16.encoder.layers.encoder_layer_11.ln_1.bias
138: vitb16.encoder.layers.encoder_layer_11.self_attention.in_proj_weight
139: vitb16.encoder.layers.encoder_layer_11.self_attention.in_proj_bias
140: vitb16.encoder.layers.encoder_layer_11.self_attention.out_proj.weight
141: vitb16.encoder.layers.encoder_layer_11.self_attention.out_proj.bias
142: vitb16.encoder.layers.encoder_layer_11.ln_2.weight
143: vitb16.encoder.layers.encoder_layer_11.ln_2.bias
144: vitb16.encoder.layers.encoder_layer_11.mlp.0.weight
145: vitb16.encoder.layers.encoder_layer_11.mlp.0.bias
146: vitb16.encoder.layers.encoder_layer_11.mlp.3.weight
147: vitb16.encoder.layers.encoder_layer_11.mlp.3.bias
148: vitb16.encoder.ln.weight
149: vitb16.encoder.ln.bias
150: vitb16.heads.0.weight
151: vitb16.heads.0.bias
=============================================================


=============================================================================================================================
Layer (type:depth-idx)                             Input Shape               Output Shape              Param #
=============================================================================================================================
VIT_B16                                            [1, 1, 224, 224]          [1, 3]                    --
├─VisionTransformer: 1-1                           [1, 1, 224, 224]          [1, 3]                    768
│    └─Conv2d: 2-1                                 [1, 1, 224, 224]          [1, 768, 14, 14]          (197,376)
│    └─Encoder: 2-2                                [1, 197, 768]             [1, 197, 768]             151,296
│    │    └─Dropout: 3-1                           [1, 197, 768]             [1, 197, 768]             --
│    │    └─Sequential: 3-2                        [1, 197, 768]             [1, 197, 768]             85,054,464
│    │    └─LayerNorm: 3-3                         [1, 197, 768]             [1, 197, 768]             1,536
│    └─Sequential: 2-3                             [1, 768]                  [1, 3]                    --
│    │    └─Linear: 3-4                            [1, 768]                  [1, 3]                    2,307
=============================================================================================================================
Total params: 85,407,747
Trainable params: 77,970,435
Non-trainable params: 7,437,312
Total mult-adds (Units.MEGABYTES): 95.40
=============================================================================================================================
Input size (MB): 0.20
Forward/backward pass size (MB): 104.09
Params size (MB): 227.63
Estimated Total Size (MB): 331.92
=============================================================================================================================